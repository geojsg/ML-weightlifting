---
title: "Weight Lifting Exercises Classification"
author: "Geojsg"
output: html_document
---

### Executive summary
The purpose is to propose a multi-class classifier able to assess the quality (5 different levels) of weight lifting exercises using data from different sensors and the related training dataset.  
First, the training dataset is cleaned, split into training and cross-validation and random forests are used on a small subset of training to find and select the most important predictors.  
Then random forests are again used but on the full training dataset to predict and evaluate the classes on cross-validation dataset before predicting them of the testing dataset.  
  
Further materials related to the dataset:  
http://groupware.les.inf.puc-rio.br/har  

### Data Loading (train & test)

```{r data_loading, cache=TRUE}
pmltrain<-read.csv("pml-training.csv",header=T)
pmltest<-read.csv("pml-testing.csv",header=T)

## dimension of the training set
dim(pmltrain)

## A look at the class variable (output)
table(pmltrain$classe)

```

The training dataset has `r nrow(pmltrain)` observations with `r ncol(pmltrain)` features and a categorical variable as output having 5 different values (multi class)

```{r te}
dim(pmltest)
```
The testing set, for which class variable needs to be predicted, has `r nrow(pmltest)` observations and also `r ncol(pmltest)` features.

### Pre-processing

First, we will remove all features having mostly (at least 95%) blank or NA values. 

```{r preproc1}
NAcol<-sapply(pmltrain,function(x) (sum(is.na(x)|x=="")/nrow(pmltrain))>0.95)
pmlc<-pmltrain[,!NAcol]
dim(pmlc)

```

Such basic cleaning enables to remove `r ncol(pmltrain)-ncol(pmlc)` features and therefore reduces our dataset to `r ncol(pmlc)` features.

Let's have a look at the remaining features names to try to identify non-relevant features.
```{r featurenames}
names(pmlc)
```
Let's have a closer look to following features which by names does not sound like useful predictors:

```{r nonrelevantfeat}
summary(pmlc[,c(1:7)])
```
As the problem is to classify the quality of the exercises using data of the sensor at disposal, we can assume that following features are not useful predictors:  
- X, as it is an incremental variable (most probably record ID)  
- user name  
- timestamp features  
- windows features  

```{r rmnonrelfeat}
## Removing non-relevant features such as user names and timestamp according to the context
pmlc<-pmlc[,-c(1:7)]
dim(pmlc)
```
Now, our dataset contains only `r ncol(pmlc)` relevant features.

### Splitting the training dataset

In order to be able to evaluate our predictions before applying on test dataset, let's split the training dataset in two: training (70%) to train our algorithm and cross-validation (30%).

```{r partition, warning=FALSE }
library(ggplot2)
library(lattice)
library(caret)
inTrain<-createDataPartition(y=pmlc$classe, p=0.7,list=FALSE)
training<-pmlc[inTrain,]
cv<-pmlc[-inTrain,]
dim(training)
dim(cv)
```

### Identifying the most important features

Let's review the top most important predictors of the dataset in order to use them 
For that, the random forest algorithm will be used but due to its computation needs over a datasets of `r ncol(training)` and `r nrow(training)`, only 30% of the training dataset (i.e. `r round(nrow(training)*0.3,0)` which is still quite significant) will be used to identify the most important features.

```{r impfeat, cache=TRUE, warning=FALSE }
library(randomForest)
trainingred<-training[sample(1:nrow(training),nrow(training)*0.3),]
fit.rfp<-train(classe~.,data=trainingred,method="rf")
varImp(fit.rfp)
```

Let's select all predictors being more than 0.15 and let's have a look at the most important

```{r selectfeat, warning=FALSE}
impo<-varImp(fit.rfp)$importance
impo$feat<-rownames(impo)
mainfeat<-impo$feat[impo$Overall>15]
mainfeat
```
We will use only these `r length(mainfeat)` features as predictors.

```{r reduced_train}
trainingimp<-training[,c(mainfeat,"classe")]
```

### Training using random forest

For the training of the data, random forest is preferred for its accuracy.

```{r trainrf, cache=TRUE}
fit.rfimp<-train(classe~.,data=trainingimp,method="rf")
fit.rfimp
```

Let's check the in-sample error rate.

```{r insample}
predtr<-predict(fit.rfimp,trainingimp)
cfMtr<-confusionMatrix(trainingimp$classe,predtr)
cfMtr
```

Actually there is no in-sample error at all as accuracy is `r cfMtr$overall[1]`, but it may overfit so let's see the result on the cross-validation subset, if there is no overfit, the accuracy should be high, close to 1.
First, we need to select only the most important features from the cross-validation subset.

```{r predictioncv}
cvimp<-cv[,c(mainfeat,"classe")]
predcv<-predict(fit.rfimp,cvimp)
cfMcv<-confusionMatrix(cvimp$classe,predcv)
cfMcv
```

The results are also quite satisfying as accuracy is `r round(cfMcv$overall[1],3)`: the algorithm generalizes well.

## Predicting the classe on testing dataset

Now let's predict the 20 classes from the test dataset and with their probabilities to get further information.
First, the test dataset is transformed in same way as the training dataset.

```{r predictions}

pmltestc<-pmltest[,!NAcol]
pmltestc<-pmltestc[,-c(1:7)]
pmltestimp<-pmltestc[,mainfeat]
pmlpred<-predict(fit.rfimp,pmltestimp)
pmlpred
## Probabilities of each classes
predict(fit.rfimp,pmltestimp,type="prob")
```

Saving each prediction in a file:

```{r savefile}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("outputs/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(pmlpred)
```

### Results

The 20 cases were perfectly predicted.