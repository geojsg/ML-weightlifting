---
title: "Weight Lifting Exercises Classification"
author: "Geojsg"
output: html_document
---

### Executive summary
The purpose of 

### Data Loading (training & test)

```{r data_loading, cache=TRUE}
pmltrain<-read.csv("pml-training.csv",header=T)
pmltest<-read.csv("pml-testing.csv",header=T)

## dimension of the training set
dim(pmltrain)

## A look at the class variable (output)
table(pmltrain$classe)

```

The training dataset has `r nrow(pmltrain)` observations with `r ncol(pmltrain)` features and a categorical variable as output having 5 different values (multi class)

```{r te}
dim(pmltest)
```
The testing set for which class variable needs to be predicted has `r nrow(pmltest)` observations and also `r ncol(pmltest)` features.

### Exploration



### Pre-processing

First, we will remove all features having mostly (at least 95%) blank or NA values. 

```{r preproc1}
NAcol<-sapply(pmltrain,function(x) (sum(is.na(x)|x=="")/nrow(pmltrain))>0.95)
pmlc<-pmltrain[,!NAcol]
dim(pmlc)

```

Such basic cleaning enables to remove `r ncol(pmltrain)-ncol(pmlc)` features and therefore reduces our dataset to `r ncol(pmlc)` features.

Let's have a look at the remaining features names to try to identify non-relevant features.
```{r featurenames}
names(pmlc)
```
Let have a closer look to following features which by names does not sound like useful predictors:

```{r nonrelevantfeat}
summary(pmlc[,c(1:7)])
```
As the problem is to classify the quality of the exercises using data of the sensor at disposal, we can assume that following features are not useful predictors:  
- X, as it is an incremental variable (most probably record ID)
- user name
- timestamp features
- windows features

```{r rmnonrelfeat}
## Removing non-relevant features such as user names and timestamp according to the context
pmlc<-pmlc[,-c(1:7)]
dim(pmlc)
```
Now, our dataset contains only `r ncol(pmlc)` relevant features.

### Splitting the training dataset

In order to be able to evaluate our predictions before applying on test dataset, let's split the training dataset in two: training (70%) to train our algorithm and cross-validation (30%).

```{r partition, warning=FALSE }
library(caret)
inTrain<-createDataPartition(y=pmlc$classe, p=0.7,list=FALSE)
training<-pmlc[inTrain,]
cv<-pmlc[-inTrain,]
dim(training)
dim(cv)
```

### Identifying the most important features

Let's review the top most important predictors of the dataset in order to use them 
For that, the random forest algorithm will be used but due to its computation needs over a datasets of `r ncol(training)` and `r nrow(training)`, only 30% of the training dataset (i.e. `r round(nrow(training)*0.3,0)` which is still quite significant) will be used to identify the most important features.

```{r impfeat, cache=TRUE, warning=FALSE }
trainingred<-training[sample(1:nrow(training),nrow(training)*0.3),]
fit.rfp<-train(classe~.,data=trainingred,method="rf")
varImp(fit.rfp)
```

Let's select all predictors being more than 0.15 and let's have a look at the most important

```{r selectfeat}
impo<-varImp(fit.rfp)$importance
impo$feat<-rownames(impo)
impo$feat[impo$Overall>15]
mainfeat<-impo$feat[impo$Overall>15]
length(mainfeat)
```
We will use only these `r length(mainfeat)` features as predictors.

```{r reduced_train}
trainingimp<-training[,c(mainfeat,"classe")]
```

### Training using random forest

Known for its accuracy and ..., random forest is the algorithm we select for the training of the data.

```{r trainrf, cache=TRUE}
fit.rfimp<-train(classe~.,data=trainingimp,method="rf")
fit.rfimp
```


Let's check the ?in sample error?

```{r}
predtr<-predict(fit.rfimp,trainingimp)
confusionMatrix(trainingimp$classe,predtr)
str(confusionMatrix(trainingimp$classe,predtr))
```

Results is quite satisfying, so let's the result on the cross-validation subset.
First, we need to select only the most important features from the cross-validation subset.

```{r prediction cv}
cvimp<-cv[,c(mainfeat,"classe")]
predcv<-predict(fit.rfimp,cvimp)
confusionMatrix(cvimp$classe,predcv)
```

The results are also quite satisfying. The algorithm generalizes well.
ROC


## Predicting the classe on testing dataset

```{r predictions}

pmltestc<-pmltest[,!NAcol]
pmltestc<-pmltestc[,-c(1:7)]
pmltestimp<-pmltestc[,mainfeat]
pmlpred<-predict(fit.rfimp,pmltestimp)
pmlpred
## to visualize probabilities
predict(fit.rfimp,pmltestimp,type="prob")
```


